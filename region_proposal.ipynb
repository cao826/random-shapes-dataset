{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4715cf53-0648-48f2-9922-9ec692d28b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchvision.models.detection import rpn\n",
    "from torchvision import transforms as T\n",
    "from Modules import image_maker, shapes, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b63377-bf77-4526-9c38-a99e79e26a65",
   "metadata": {},
   "source": [
    "## Generating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d42b39f-9d8b-43dd-a90f-e7c83560e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's make an example 512 x 512 image\n",
    "\"\"\"\n",
    "canvas_size = (512, 512)\n",
    "color_picker = colors.ColorPicker(\n",
    "    colors.KNOWN_COLORS,\n",
    "    colors.default_hls_settings\n",
    ")\n",
    "\n",
    "img_generator = image_maker.ImageMaker(image_shape=canvas_size,\n",
    "                                       color_picker=color_picker,\n",
    "                                       number_of_colors=4,\n",
    "                                       shapes = [\n",
    "                                           shapes.Triangle(),\n",
    "                                           shapes.Circle(\n",
    "                                               specifications={\n",
    "                                                   'min_size': 80,\n",
    "                                                   'max_size': 220\n",
    "                                               }),\n",
    "                                           shapes.Square(\n",
    "                                               specifications={\n",
    "                                                   'min_size': 35,\n",
    "                                                   'max_size': 250\n",
    "                                               })\n",
    "                                       ],\n",
    "                                      )\n",
    "\n",
    "def make_image():\n",
    "    image_meta = img_generator()\n",
    "    image = image_maker.construct_image(image_meta)\n",
    "    return image, image_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e686a4-aa91-4086-a545-b9250d6e66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_image_meta = make_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ceb839b-4fc9-4071-8ddb-7d1006933aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fce5254a4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.axis('off')\n",
    "ax.imshow(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff2792-bd9b-4e81-a4ea-b77f9ddc69a7",
   "metadata": {},
   "source": [
    "## Instantiating a pretrained model to create a feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01637d1c-69e4-41e8-9001-b7db4f6003fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/carlosolivares/.cache/torch/hub/pytorch_vision_v0.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 has 3 layers\n"
     ]
    }
   ],
   "source": [
    "model_repo = 'pytorch/vision:v0.10.1'\n",
    "\n",
    "vgg16 = torch.hub.load(repo_or_dir=model_repo,\n",
    "                       model='vgg16')\n",
    "layer_count = 0\n",
    "for layer in vgg16.children():\n",
    "    layer_count += 1\n",
    "print(f\"vgg16 has {layer_count} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e3759-5900-4253-90ad-e59d308dd645",
   "metadata": {},
   "source": [
    "Clearly the model has more than three layers. What actually happens is that they group the related steps together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d36f80-1ed4-48bc-ad2f-845b40bda178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE VGG16 MODEL\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "FEATURE MAP MAKER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/carlosolivares/.cache/torch/hub/pytorch_vision_v0.10.1\n",
      "/Users/carlosolivares/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Users/carlosolivares/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create the vgg16 based feature map creator\n",
    "\"\"\"\n",
    "n\n",
    "\n",
    "print(\"COMPLETE VGG16 MODEL\")\n",
    "print(vgg16)\n",
    "print(\"FEATURE MAP MAKER\")\n",
    "print(vgg16_featuremapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888a07e-fc25-4a23-91f2-e1a5fe4a9101",
   "metadata": {},
   "source": [
    "## making a feature map to pass to the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84530555-a704-41f2-bbe6-318f654eab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "standard_transform = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605c3e28-bd85-4b82-9864-04f230ec6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/carlosolivares/.cache/torch/hub/pytorch_vision_v0.10.1\n"
     ]
    }
   ],
   "source": [
    "feature_mapper = vgg16_featuremapper()\n",
    "image_for_input = standard_transform(example_image)\n",
    "feature_map = feature_mapper(image_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54357727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature map has shape torch.Size([512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The feature map has shape {feature_map.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff122fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Putting all of this in an ImageList \n",
    "\"\"\"\n",
    "\n",
    "image_list = ImageList(tensors=image_for_input, image_sizes=[(512,512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74c8eef-a102-4749-a51d-1504253a74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = AnchorGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a447cbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 256, 512),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_generator.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb06b28-6ab9-42e3-8a1b-6945c74fccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_is_this = anchor_generator(image_list, [feature_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad296641-b97c-4ec3-8513-0d4ebad65fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here ye, here ye! This object is the output of pytorch's anchor generator!\n",
      "It is of type: <class 'list'>\n",
      "the 0 element of this list is of type<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Here ye, here ye! This object is the output of pytorch's anchor generator!\")\n",
    "print(f\"It is of type: {type(what_is_this)}\")\n",
    "for i, thing in enumerate(what_is_this):\n",
    "    print(f\"the {i} element of this list is of type{type(thing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5938fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304, 4])\n"
     ]
    }
   ],
   "source": [
    "print(what_is_this[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb14ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ -91.,  -45.,   91.,   45.],\n",
      "        [-181.,  -91.,  181.,   91.],\n",
      "        [-362., -181.,  362.,  181.],\n",
      "        ...,\n",
      "        [ 435.,  389.,  525.,  571.],\n",
      "        [ 389.,  299.,  571.,  661.],\n",
      "        [ 299.,  118.,  661.,  842.]])]\n"
     ]
    }
   ],
   "source": [
    "print(what_is_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "166617c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_generator.num_anchors_per_location()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c70e174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anchor_generator.num_anchors_per_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc68daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sj/ly263hxx0vz_tvymz1bwjkgm0000gn/T/ipykernel_45336/1598600101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../coco\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../coco\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/basic-pytorch/lib/python3.7/site-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     32\u001b[0m     ) -> None:\n\u001b[1;32m     33\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "umm = torchvision.datasets.CocoDetection(\"../coco\", annFile=\"../coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ee610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic-pytorch]",
   "language": "python",
   "name": "conda-env-basic-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
